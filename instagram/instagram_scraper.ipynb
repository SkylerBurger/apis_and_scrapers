{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instagram Scraper\n",
    "\n",
    "This notebook is an exploration of a few difficulties in web scraping Instagram.\n",
    "\n",
    "If you like this example, check out my [`insta_builder.py`](../modules/insta_builder.py) module where I implement the Builder design pattern to create representations of an Instagram profile. This example could be extended to handle profile from other social media platforms by creating a new builder and profile class for each platform.\n",
    "\n",
    "## Challenges:\n",
    "\n",
    "1. GET requests to Instagram return executable JavaScript rather than HTML content.\n",
    "  - To get past this issue I learned how to use Selenium which executes the JavaScript included in the response from Instagram just as a browser would. This gathers and renders the actual HTML content so I can scrape it.\n",
    "2. Instagram has a soft login-wall to keep clients that are not logged in from viewing the full extent of a profile's public content.\n",
    "  - When researching how to scroll through a page of unknown length in Selenium, I came across an approach that manipulates the `window` object directly to continue scrolling. Luckily, this direct approach works even after Instagram renders their soft login-wall to the page.\n",
    "3. Instagram dynamically populates and depopulates images from the browser as you scroll through a profile.\n",
    "  - To make sure that I capture all of the images as I scroll, I set up an algorithm that takes a 'snapshot' of the current HTML content of the page and then scrolls to the bottom of the page. Scrolling to the bottom of the page causes new content to render and old content to depopulate from the page. It then repeats this process of taking a snapshot and scrolling until it notices that the bottom of the page is no longer extending. I then extract all the image tags from the HTML snapshots and remove duplicates by running them through a set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from random import randint\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter an Instagram profile name as a string below\n",
    "# And then un-comment the line\n",
    "\n",
    "# profile_name = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "instagram_url = f'https://www.instagram.com/{profile_name}'\n",
    "max_scroll_secs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = []\n",
    "def snapshot(browser):\n",
    "    global snapshots\n",
    "    snapshots.append(browser.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribution for scrolling mechanism:\n",
    "# https://stackoverflow.com/questions/20986631/how-can-i-scroll-a-web-page-using-selenium-webdriver-in-python#27760083\n",
    "\n",
    "def scroll_and_snapshot(browser, max_scroll_secs):\n",
    "    SCROLL_PAUSE_TIME = 1\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    end_time = time.time() + max_scroll_secs\n",
    "    \n",
    "    while time.time() < end_time:\n",
    "        # Scroll down to bottom\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height <= last_height:\n",
    "            snapshot(browser)\n",
    "            break\n",
    "        # Create HTML snapshot\n",
    "        snapshot(browser)\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_image_tags(snapshots):\n",
    "    \n",
    "    image_tags = []\n",
    "    \n",
    "    for html in snapshots:\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        image_tags += soup.find_all('img', class_='FFVAD')\n",
    "        \n",
    "    return set(image_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture(snapshots, browser):\n",
    "        \n",
    "    image_tags = collect_image_tags(snapshots)\n",
    "\n",
    "    for index, image in enumerate(image_tags):\n",
    "        browser.get(image['src'])\n",
    "        images = browser.find_elements_by_tag_name('img')\n",
    "        # Edit file name f-string below  as needed\n",
    "        images[0].screenshot(f'./screenshot_{index}.png')\n",
    "        wait_time = randint(1, 2)\n",
    "        time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Image Links Captured: 395\n"
    }
   ],
   "source": [
    "# If you need to download/update chromedriver\n",
    "# https://sites.google.com/a/chromium.org/chromedriver/\n",
    "\n",
    "browser = webdriver.Chrome('../utilities/chromedriver')\n",
    "# browser.implicitly_wait(10)\n",
    "browser.get(instagram_url)\n",
    "\n",
    "# Find the 'show more' button and click it\n",
    "browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "button = browser.find_element_by_class_name('z4xUb')\n",
    "button.click()\n",
    "\n",
    "scroll_and_snapshot(browser, max_scroll_secs)\n",
    "\n",
    "# ONLY call line below if you want to risk saving a TON of images\n",
    "# capture(snapshots, browser)\n",
    "\n",
    "# 2 lines below report # of tags captured w/o saving images\n",
    "images = collect_image_tags(snapshots)\n",
    "print(f'Image Links Captured: {len(images)}')\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('apis-and-scrapers': venv)",
   "language": "python",
   "name": "python38264bitapisandscrapersvenvf2140220de3048d48d920259101581f4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}